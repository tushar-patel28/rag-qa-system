{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# RAG-Based Question Answering System\n",
    "## Using LangChain, HuggingFace, and FAISS\n",
    "\n",
    "This notebook demonstrates a Retrieval-Augmented Generation (RAG) system that answers questions based on PDF documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 1: Import Libraries\n",
    "# --------------------------\n",
    "import os\n",
    "from getpass import getpass\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 2: Set Hugging Face Token (Securely)\n",
    "# --------------------------\n",
    "# Option 1: Use environment variable (recommended for production)\n",
    "# Set this in your terminal before running: export HUGGINGFACEHUB_API_TOKEN=\"your_token_here\"\n",
    "\n",
    "# Option 2: Input token securely (recommended for notebooks)\n",
    "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
    "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass(\"Enter your HuggingFace API token: \")\n",
    "else:\n",
    "    print(\"Using HuggingFace token from environment variable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 3: Configuration Parameters\n",
    "# --------------------------\n",
    "CONFIG = {\n",
    "    \"pdf_path\": \"A_Brief_Introduction_To_AI.pdf\",\n",
    "    \"chunk_size\": 500,\n",
    "    \"chunk_overlap\": 100,\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"llm_model\": \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"temperature\": 0.5,\n",
    "    \"max_tokens\": 512,\n",
    "    \"retriever_k\": 4  # Number of documents to retrieve\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-pdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 4: Load and Chunk the PDF\n",
    "# --------------------------\n",
    "try:\n",
    "    print(f\"Loading PDF: {CONFIG['pdf_path']}\")\n",
    "    loader = PyPDFLoader(CONFIG['pdf_path'])\n",
    "    pages = loader.load()\n",
    "    print(f\"✓ Loaded {len(pages)} pages\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CONFIG['chunk_size'],\n",
    "        chunk_overlap=CONFIG['chunk_overlap']\n",
    "    )\n",
    "    documents = text_splitter.split_documents(pages)\n",
    "    print(f\"✓ Split into {len(documents)} chunks\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: PDF file '{CONFIG['pdf_path']}' not found.\")\n",
    "    print(\"Please ensure the PDF file is in the same directory as this notebook.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading PDF: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 5: Generate Embeddings and Create Vector Store\n",
    "# --------------------------\n",
    "try:\n",
    "    print(f\"Creating embeddings using {CONFIG['embedding_model']}...\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=CONFIG['embedding_model'])\n",
    "    \n",
    "    print(\"Building FAISS vector store...\")\n",
    "    db = FAISS.from_documents(documents, embedding_model)\n",
    "    print(\"✓ Vector store created successfully\")\n",
    "    \n",
    "    # Optional: Save the vector store for future use\n",
    "    # db.save_local(\"faiss_index\")\n",
    "    # print(\"✓ Vector store saved to disk\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating vector store: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-llm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 6: Set Up LLM and RAG Chain\n",
    "# --------------------------\n",
    "try:\n",
    "    print(f\"Initializing LLM: {CONFIG['llm_model']}...\")\n",
    "    llm = HuggingFaceEndpoint(\n",
    "        repo_id=CONFIG['llm_model'],\n",
    "        temperature=CONFIG['temperature'],\n",
    "        max_new_tokens=CONFIG['max_tokens']\n",
    "    )\n",
    "    \n",
    "    print(\"Creating RAG chain...\")\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=db.as_retriever(search_kwargs={\"k\": CONFIG['retriever_k']}),\n",
    "        return_source_documents=True\n",
    "    )\n",
    "    print(\"✓ RAG chain ready\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error setting up RAG chain: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helper-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 7: Question Answering Helper Function\n",
    "# --------------------------\n",
    "def ask_question(query, show_sources=True, max_source_length=500):\n",
    "    \"\"\"\n",
    "    Ask a question and get an answer based on the PDF content.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The question to ask\n",
    "        show_sources (bool): Whether to display source documents\n",
    "        max_source_length (int): Maximum characters to show from each source\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = qa_chain.invoke({\"query\": query})\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(f\"Question: {query}\")\n",
    "        print(\"=\"*80)\n",
    "        print(f\"\\nAnswer:\\n{result['result']}\\n\")\n",
    "        \n",
    "        if show_sources and result.get(\"source_documents\"):\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(\"Source Documents:\")\n",
    "            print(\"=\"*80)\n",
    "            for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "                print(f\"\\n--- Source {i} ---\")\n",
    "                content = doc.page_content[:max_source_length]\n",
    "                if len(doc.page_content) > max_source_length:\n",
    "                    content += \"...\"\n",
    "                print(content)\n",
    "                \n",
    "                # Show metadata if available\n",
    "                if doc.metadata:\n",
    "                    print(f\"\\nMetadata: {doc.metadata}\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-usage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Step 8: Example Questions\n",
    "# --------------------------\n",
    "# Example 1: Basic question\n",
    "ask_question(\"What is Artificial Intelligence?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: More specific question\n",
    "ask_question(\"What are the subfields of AI mentioned in the document?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "example-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Question without showing sources\n",
    "ask_question(\"How does machine learning work?\", show_sources=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interactive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------\n",
    "# Interactive Question Answering\n",
    "# --------------------------\n",
    "# Uncomment the following lines to enable interactive mode\n",
    "\n",
    "# while True:\n",
    "#     user_query = input(\"\\nEnter your question (or 'quit' to exit): \")\n",
    "#     if user_query.lower() in ['quit', 'exit', 'q']:\n",
    "#         print(\"Goodbye!\")\n",
    "#         break\n",
    "#     ask_question(user_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
